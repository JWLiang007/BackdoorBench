---
# For CUDA convolution: Turn off deterministic algorithms and turn on benchmarking.
# The settings will speedup training, while introducing nondeterministic behaviors.
# See https://pytorch.org/docs/stable/notes/randomness.html for detailed informations.
seed:
  seed: 100
  deterministic: False
  benchmark: True
dataset_dir: ~/dataset/cifar-10/cifar-10-batches-py  # Contain the sub-string `cifar`.
num_classes: 10
# Logs will be saved in `saved_dir` and checkpoints will be saved in the `storage_dir`.
# Please make sure the `saved_dir` and `storage_dir` exist in the project root.
saved_dir: ./saved_data
storage_dir: ./storage
prefetch: True  # turn on prefetch mode will speedup io
# First, apply `pre` transformations to images before adding triggers (if needed).
# And then, apply `primary` and `remaining` transformations sequentially.
transform:
  pre: null
  train:
    primary:
      random_crop:
        size: 32
        padding: 4
      random_horizontal_flip:
        p: 0.5
    remaining:
      to_tensor: True
      normalize:
        mean: [0.4914, 0.4822, 0.4465]
        std: [0.2023, 0.1994, 0.2010]
  test:
    primary: null
    remaining:
      to_tensor: True
      normalize:
        mean: [0.4914, 0.4822, 0.4465]
        std: [0.2023, 0.1994, 0.2010]
backdoor:
  poison_ratio: 0.05
  target_label: 3
  badnets:
    trigger_path: ./data/trigger/cifar_1.png
loader:
  batch_size: 128
  num_workers: 4
  pin_memory: True
network:
  resnet18_cifar:
    num_classes: 10
sync_bn: True  # synchronized batch normalization (distributed data parallel)
criterion:
  cross_entropy:
    reduction: "mean"
optimizer:
  SGD:
    weight_decay: 5.e-4
    momentum: 0.9
    lr: 0.1  # 0.1 * batch_size / 128
lr_scheduler:
  multi_step:
    milestones: [100, 150]
num_epochs: 200